{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "诸如股票, 小说文本等数据并不是独立同分布, 而是前后相关.这种序列数据与之前的图像/表格数据都不一样. \n",
    "* 回顾统计工具: \n",
    "- 自回归模型 e.g.前一个词预判后一个词 $X^T =a^{T-1} X^{T-1}+...$\n",
    "- 马尔科夫模型\n",
    "- 因果关系\n",
    "- 训练/预测 \n",
    "* 循环神经网络原理\n",
    "序列到序列模型原理\n",
    "lstm原理\n",
    "transformer原理\n",
    "attention原理\n",
    "* 文本数据嵌入( what is embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e.g.动力系统 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Recurrent neural networks, or RNNs (Rumelhart et al., 1986a), are a family of neural networks for processing **sequential data**. Much as a convolutional network is a neural network that is specialized for processing a grid of values X such as an image, a recurrent neural network is a neural network that is specialized for processing a sequence of values $x^{(1)}, . . . , x^{(τ)}$.  *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../Week5%20NLP/img/unfold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h: hidden state**\n",
    "$$ h^{(t)}=f(h^{(t-1)},x^{(t)};\\theta)$$\n",
    "序列长度 & 信息保存 lossy (-> encoder)\n",
    "$$\n",
    "\\begin{align}\n",
    "h^{(t)}&=g^{(t)}(x^{(t)},x^{(t-1)},...,x^{(1)}) \\\\\n",
    "&=f(h^{(t-1)},x^{(t)};\\theta)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "* 优点:\n",
    "- 统一输入大小\n",
    "- 共享f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 的多种写法\n",
    "* 每个timestep 一个output, h-h之间有关联10.3\n",
    "* 每个timestep 一个output, o-h之间有关联10.4\n",
    "* h-h关联, 读入完整序列得到一个结果      10.5\n",
    "\n",
    "> * 每个timestep 一个output, h-h之间有关联10.3\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{(t)}&=b+WH^{(t-1)}+Ux^{(t)},\\\\\n",
    "h^{(t)}&=tanh(a^{(t)}),\\\\\n",
    "o^{(t)}&=c+Vh^{(t)},\\\\\n",
    "{ \\hat y}^{(t)}&=softmax(o^{(t)}),\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ & L({\n",
    "        x^{(1)},...,x^{(\\tau)}\n",
    "    },\n",
    "    {\n",
    "        y^{(t)},...,y^{(\\tau)}\n",
    "    }) \\\\\n",
    "=&\\sum_t L^{(t)} \\\\\n",
    "=& -\\sum_t logp_{model}(y^{(t)}|{x^{(1)},...,x^{(t)}}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "where $p_{model}(y^{(t)}|{x^{(1)},...,x^{(t)}})$ is given by reading the entry for $y^{(t)}$ from the model’s output vector ${\\hat y}^{(t)}$\n",
    "Computing the gradient of this loss function with respect to the parameters is an expensive operation\n",
    "runtime, memory cost \n",
    "**back-propagation through time (BPTT)** The back-propagation algorithm applied to the unrolled graph with $O(\\tau)$ cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 10.4 每个timestep 一个output, o-h之间有关联;\n",
    ">\n",
    ">除非o非常高维并且丰富, 否则o通常不含重要的过去信息(可能让RNN能力变弱), 但是可能有利于让第t步独立于t-1步, 允许更多平行(在训练期间)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 10.5 可以用来总结序列/构建固定大小的表征(作为后续处理的输入, 相当于embedding嵌入)h-h关联, 读入完整序列得到一个结果      10.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Teacher Forcing and Networks with Output Recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Models that have recurrent connections from their outputs leading back into the model may be trained with teacher forcing. Teacher forcing is a procedure that emerges from the maximum likelihood criterion, in which during training the model receives the ground truth output y(t) as input at time t + 1.\n",
    "用两步序列检验:\n",
    "$$\n",
    "\\begin{align}\n",
    " \\ & logp(y^{(1)},y^{(2)}|x^{(1)},x^{(2)})\\\\\n",
    "= & logp(y^{(2)}|y^{(1)},x^{(1)},x^{(2)})+logp(y^{(1)}|x^{(1)},x^{(2)})\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "In this example, we see that at time t= 2, the model is trained to maximize the conditional probability of $y^(2)$ given both the x sequence so far and the previous y value from the training set. Maximum likelihood thus speciﬁes that during training,rather than feeding the model’s own output back into itself, these connections should be fed with the target values specifying what the correct output should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 缺点:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 依赖标签数据,cross-domain能力弱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 解决:\n",
    "- **Beam Search**(集束搜索)\n",
    "在预测单词这种离散值的输出时，一种常用方法是对词表中每一个单词的预测概率执行搜索，生成多个候选的输出序列。\n",
    "这个方法常用于机器翻译(MT)等问题，以优化翻译的输出序列。\n",
    "beam search是完成此任务应用最广的方法，通过这种启发式搜索(heuristic search)，可减小模型学习阶段performance与测试阶段performance的差异\n",
    "\n",
    "- 有计划地学习(**Curriculum Learning**)\n",
    "如果模型预测的是实值(real-valued)而不是离散值(discrete value)，那么beam search就力不从心了。\n",
    "因为beam search方法仅适用于具有离散输出值的预测问题，不能用于预测实值（real-valued）输出的问题\n",
    "有计划地学习的意思就是: 使用一个概率p去选择使用ground truth的输出y(t)还是前一个时间步骤模型生成的输出h(t)作为当前时间步骤的输入x(+1)。\n",
    "这个概率p会随着时间的推移而改变，这就是所谓的计划抽样(scheduled sampling)\n",
    "训练过程会从force learning开始，慢慢地降低在训练阶段输入ground truth的频率\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「Alanaker」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/qq_30219017/article/details/89090690"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../Week5%20NLP/img/10.6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 全连接缺点:\n",
    ">\n",
    "> 内存/效率\n",
    ">\n",
    "> ->RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Networks as Directed Graphical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依赖条件分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 考虑上下文语境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g上下文语境填空/做视频"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder&Sequence-to-Sequence Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder\n",
    "机器翻译是序列转换模型的一个核心问题， 其输入和输出都是长度可变的序列。 为了处理这种类型的输入和输出， 我们可以设计一个包含两个主要组件的架构： $\\\\$第一个组件是一个编码器（**encoder**）： 它接受一个长度可变的序列作为输入， 并将其转换为具有固定形状的编码状态。$\\\\$ 第二个组件是解码器（**decoder**）： 它将固定形状的编码状态映射到长度可变的序列。 这被称为编码器-解码器（**encoder-decoder**）架构.$\\\\$ $\\\\$\n",
    "以英语到法语的机器翻译为**例**： 给定一个英文的输入序列： “They”“are”“watching”“.”。 $\\\\$首先，这种“编码器－解码器”架构将长度可变的输入序列编码成一个“状态”， 然后对该状态进行解码， 一个词元接着一个词元地生成翻译后的序列作为输出： “Ils”“regordent”“.”。 $\\\\$ 由于“编码器－解码器”架构是形成后续章节中不同序列转换模型的基础， 因此本节将把这个架构转换为接口方便后面的代码实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入序列的信息被编码到循环神经网络编码器的隐状态中。 为了连续生成输出序列的词元， 独立的循环神经网络解码器是基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元。\n",
    "![](../Week5%20NLP/img/encoder.png)\n",
    " 在循环神经网络解码器的初始化时间步，有两个特定的设计决定： 首先，特定的“<bos>”表示序列开始词元，它是解码器的输入序列的第一个词元。 特定的“<eos>”表示序列结束词元。 一旦输出序列生成此词元，模型就会停止预测。其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Gradient in a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain some intuition for how the BPTT algorithm behaves, we provide an example of how to compute gradients by BPTT for the RNN equations\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{(t)}&=b+WH^{(t-1)}+Ux^{(t)},\\\\\n",
    "h^{(t)}&=tanh(a^{(t)}),\\\\\n",
    "o^{(t)}&=c+Vh^{(t)},\\\\\n",
    "{ \\hat y}^{(t)}&=softmax(o^{(t)}),\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "assume that the loss is the negative log-likelihood of the true targety\n",
    "$$\\frac{\\partial L}{\\partial L^{(t)}}=1  $$\n",
    "\n",
    "$$\n",
    "(\\nabla_{o^{(t)}}L)_i =\\frac{\\partial L}{\\partial o_{i}^{(t)}}=\n",
    "\\frac{\\partial L}{\\partial L^{(t)}} \\frac{\\partial L^{(t)}}{\\partial o_{i}^{(t)}}={\\hat y}_{i}^{(t)}-\\mathbb{1}_{i=y^{(t)}}\n",
    "$$\n",
    "最后一步$ \\tau $,h只有o一个下降项, 所以梯度:\n",
    "$$\n",
    "\\nabla_{h^{(\\tau)}}L=V^T \\nabla_{o^{(t)}}L\n",
    "$$\n",
    "向前迭代, $h^{(t)}$有$o^{(t)}$和$h^{(t+1)}$两个下降项, 梯度是:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{o^{(t)}}L=& (\\frac{\\partial h^{(t+1)}}{\\partial h^{(t)}})^T (\\nabla_{h^{(t+1)}}L)+(\\frac{\\partial o^{(t)}}{\\partial h^{(t)}})^T (\\nabla_{o^{(t)}}L)\\\\\n",
    "\\ =&W^T diag(1-(h^{(t+1)})^2 )(\n",
    "\\nabla_{h^{(t+1)}}L)+V^T (\\nabla_{o^{(t)}}L)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "diag...Jacobian of the hyperbolic tangent associated with the hidden unit i at time t + 1;\n",
    "* 考虑了每个节点的影响, 但是往往希望考虑权重矩阵W的偏导. 于是引入哑变量$W^{(t)}$ 是W的拷贝但是每个$W^{(t)}$只在第t步使用. 于是可以求偏导:\n",
    "![](../Week5%20NLP/img/partial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 当组合许多非线性函数(如这里所示的线性tanh layer)时，结果是高度非线性的，通常大多数值与一个微小的导数相关，一些值与一个大的导数相关，并且在增加和减少之间有许多交替。在这里，我们绘制了一个100维隐藏状态的线性投影到一个单维，绘制在它们的轴上。x轴是初始状态在100维空间中沿随机方向的坐标。因此，我们可以把这个图看作是一个高维函数的线性横截面。该图显示了每个时间步之后的函数，或者等价地，在转换函数被组成的每个次数之后."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度爆炸 -> 门控 / 梯度剪裁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 门控循环单元（GRU）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为循环使用W, 梯度可能会爆炸/衰减到0.$\\\\$\n",
    "影响&可能情况:\n",
    "* 早期观测值对预测所有未来观测值具有非常重要的意义。 考虑一个极端情况，其中第一个观测值包含一个校验和， 目标是在序列的末尾辨别校验和是否正确。 在这种情况下，第一个词元的影响至关重要。 我们希望有某些机制能够在一个记忆元里**存储重要的早期信息**。 如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度， 因为它会影响所有后续的观测值。\n",
    "\n",
    "* 一些词元没有相关的观测值。 例如，在对网页内容进行情感分析时， 可能有一些辅助HTML代码与网页传达的情绪无关。 我们希望有一些机制来**跳过**隐状态表示中的此类词元。\n",
    "\n",
    "* 序列的各个部分之间存在逻辑中断。 例如，书的章节之间可能会有过渡存在， 或者证券的熊市和牛市之间可能会有过渡存在。 在这种情况下，最好有一种方法来**重置我们的内部状态表示**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 重置门（reset gate）允许我们控制“可能还想记住”的过去状态的数量； \n",
    "* 更新门（update gate）将允许我们控制新状态中有多少个是旧状态的副本。\n",
    "* 我们把它们设计成(0,1)区间中的向量， 这样我们就可以进行凸组合。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## long-short-term memory，LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">细胞彼此之间循环连接，取代了普通循环网络中通常隐藏的单元。输入特征是用一个规则的人工神经元单元计算的。如果s型输入门允许，它的值可以累积到状态中。状态单元具有线性自环，其权重由遗忘门控制。电池的输出可以通过输出门切断。所有的门控单元都具有s形非线性，而输入单元可以具有任何压扁非线性。状态单元也可以用作门控单元的额外输入。黑色方块表示单个时间步长的延迟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " >在极端陡峭的悬崖附近，梯度裁剪可以使梯度下降更加合理。这些陡峭的悬崖通常出现在循环网络近似线性表现的地方附近。悬崖在时间步长的数量上呈指数级陡峭，因为权重矩阵在每个时间步长上乘以它自己一次。(左)不加坡度裁剪的梯度下降越过这个小峡谷的底部，然后从悬崖表面接收一个非常大的梯度。巨大的梯度灾难性地将参数推到了图的轴线之外。(右)梯度下降与梯度裁剪对悬崖的反应更温和。当它爬上悬崖时，步长是有限的，这样它就不会被推离靠近解决方案的陡峭区域。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
